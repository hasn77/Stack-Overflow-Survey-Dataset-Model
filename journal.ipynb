{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db55cacd",
   "metadata": {},
   "source": [
    "## ____Stack Overflow Survey ML Project - Learning Journal____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW DATA\n",
    "  # └──> EDA (distributions, correlations, plots, intuition)\n",
    "    #     └──> Preprocessing (handle missing, encode categoricals, bin experience, log salary)\n",
    "      #        └──> Transformers & Pipeline (reusable, scalable, leak-proof)\n",
    "       #             └──> Scaling (StandardScaler)\n",
    "        #                  └──> Modeling (linear regression on log(salary))\n",
    "         #                       └──> Evaluation & Interpretation\n",
    "          #                            └──> Journal documentation & insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b977e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW DATA\n",
    "  # └──> train_test_split\n",
    "    #     ├──> fit transformers only on train\n",
    "      #   └──> apply transformations on train & test separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238df457",
   "metadata": {},
   "source": [
    "#### ___Project Overview___\n",
    "- Dataset: Stack Overflow 2023 Survey\n",
    "- Goal: __Predicting Yearly Salary (ConvertedComppYearly)__\n",
    "- Key Learning Objectives: Apply Chapter 2 concepts, feature engineering practice, robust pipeline building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff3543",
   "metadata": {},
   "source": [
    "#### ___Target Selection___\n",
    "- **Decision**: ConvertedCompYearly as regression target\n",
    "- **Why**: \n",
    "  - 48K samples, reasonable distribution\n",
    "  - Median $75K aligns with industry knowledge\n",
    "  - Rich feature set for prediction\n",
    "- **Challenges identified**: \n",
    "  - Extreme outliers need handling\n",
    "  - ~46% missing values\n",
    "  - Need currency/location normalization strategy\n",
    "- **Next**: Explore feature relationships and outlier handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07b6a3",
   "metadata": {},
   "source": [
    "####  ___Documentation___\n",
    "\n",
    "#### __Data Exploration__\n",
    "- **Decision**: Checked the dataset for missing values, saw statistical descriptions of each variable.  \n",
    "- **Why**: To find out a reasonable target variable. \n",
    "- **Alternatives considered**: N/A\n",
    "- **Outcome**: Most of the variables are objects/categorical. The model would require clever data preprocessing to find out only relevant variable for the target. Then onwards clever feature engineering would help to build a strong model.\n",
    "\n",
    "#### __Feature Selection__\n",
    "- **Features**: EdLevel, YearsCode, YearsCodePro, DevType, OrgSize, TechList, LanguageHaveWorkedWith, PlatformHaveWorkedWith, WebframeHaveWorkedWith, ToolsTechHaveWorkedWith, WorkExp, Industry, ProfessionalTech.\n",
    "- **Statistics**: 12 out of 13 features are object/categorical mostly highly cordinal. \n",
    "- **Choice**: Based on the tech domain intuition, these features are most likely to relate the most and can be the drivers of tech salaries. \n",
    "- **Challenges**: None so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481cd84",
   "metadata": {},
   "source": [
    "### **Feature Engineering Strategy**\n",
    "\n",
    "<details>\n",
    "  <summary><strong>📂 Click to expand details</strong></summary>\n",
    "\n",
    "---\n",
    "\n",
    "##  Topic: Advanced Feature Engineering for High-Cardinality Categorical Variables\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem Context**\n",
    "Working with the Stack Overflow survey data:\n",
    "- **13 variables** (12 categorical, 1 numerical)\n",
    "- **~89,000 rows**\n",
    "\n",
    "The main challenge:  \n",
    " High-cardinality categorical variables like `TechList` and `LanguageHaveWorkedWith` could explode into **thousands of sparse features** if naïvely one-hot encoded.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Key Insights Discovered**\n",
    "\n",
    "**1️ The Sparse Matrix Strategy**\n",
    "- **Problem:**  \n",
    "  One-hot encoding all 12 categoricals creates potentially **10K+ features** with **99%+ zeros**.\n",
    "- **Solution:**  \n",
    "  Use sklearn’s sparse matrix support with `feature_names_out` for interpretability.\n",
    "- **Why it works:**  \n",
    "  Sparse matrices store only non-zero values, reducing memory usage by **90-95%**.\n",
    "\n",
    "---\n",
    "\n",
    "**Hierarchical Feature Engineering Approach**\n",
    "Instead of flat one-hot encoding:\n",
    "-  **Stack-level features:** Group technologies into meaningful categories (frontend, backend, data science).\n",
    "-  **Technology-level features:** Preserve granular signals for high-impact individual technologies.\n",
    "\n",
    "---\n",
    "\n",
    "**Salary-Proportional Weighting Strategy**\n",
    "- **Core Problem:**  \n",
    "  Not all technologies within a stack equally impact salary.\n",
    "- **Solution:**  \n",
    "  Weight features **proportionally to salary impact**:\n",
    "\n",
    "- **Rationale:**  \n",
    "Retains genuine salary signals from both high-paying outliers and common technologies.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Technical Decisions Made**\n",
    "\n",
    "- **Weighting Approach:**  \n",
    "Chose raw salary differences over RBF smoothing for now — to maintain interpretability.  \n",
    " Will revisit after performance tests.\n",
    "\n",
    "- **Experimental Plan:**  \n",
    "1. Analyze technology → salary relationships  \n",
    "2. Implement proportional weighting  \n",
    "3. Benchmark against simple stack groups  \n",
    "4. Measure correlation & regression metrics  \n",
    "5. Then build a robust transformer for the production pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Links to Chapter 2 Concepts**\n",
    "\n",
    "-  **Feature Engineering Pipelines:**  \n",
    "Following Aurélien’s Ch.2 by separating **experimental exploration from production steps**.\n",
    "\n",
    "-  **Handling Categorical Variables:**  \n",
    "Moving beyond basic one-hots to **domain-driven encodings** that capture business realities.\n",
    "\n",
    "-  **Taming the Curse of Dimensionality:**  \n",
    "Recognizing how high-cardinality categoricals inflate feature space and strategically compressing it.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Next Steps**\n",
    "1. Implement salary analysis per technology  \n",
    "2. Build proportional weighting system  \n",
    "3. Create experimental features & test correlations  \n",
    "4. Compare against baseline stack grouping  \n",
    "5. Document insights before pipeline production.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Key Takeaway**\n",
    "> The best feature engineering blends **domain expertise** (understanding real tech clusters)  \n",
    "> with **statistical rigor** (weighting by salary impact).  \n",
    "> This is where **human judgment becomes irreplaceable** in an ML pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f5dc0",
   "metadata": {},
   "source": [
    "### ___EDA___\n",
    "\n",
    "#####  **Insight**\n",
    "After exploratory scatter plots of `YearsCode` vs `ConvertedCompYearly`, we observed extreme variance and no clear linear relationship.  \n",
    "This confirmed that raw years of experience does **not translate directly into salary** due to multiple hidden confounders (role, location, negotiation, industry).\n",
    "\n",
    "---\n",
    "\n",
    "#####  **Decision**\n",
    "- **1. Log-transform salary**  \n",
    "  To stabilize variance and interpret coefficients in percentage changes, we applied:\n",
    "  \n",
    "  LogSalary = log1p(ConvertedCompYearly)\n",
    "\n",
    "This reduces the impact of extreme salary outliers and makes relationships more linear.\n",
    "\n",
    "- **2. Bucketize years of experience**  \n",
    "Recognizing the diminishing returns of experience, we plan to group `YearsCode` into meaningful categories (e.g., 0-2 yrs, 3-5 yrs, etc).  \n",
    "This approach captures non-linear experience effects and prevents outlier distortion.\n",
    "\n",
    "---\n",
    "\n",
    "#####  **Next step**\n",
    "- Define logical buckets for `YearsCode` and `YearsCodePro` based on both:\n",
    "- **Domain intuition** (typical junior, mid, senior ranges), and\n",
    "- **Actual data distributions** (observed quantiles).\n",
    "- Explore pivot plots of average `LogSalary` vs experience buckets to confirm expected patterns.\n",
    "- Encode these buckets as categorical features in our modeling pipeline.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55bcc89",
   "metadata": {},
   "source": [
    "### _____Handling Multi-Label Categorical Fields_____\n",
    "\n",
    "- **Problem:**  \n",
    "  Several columns (e.g., `LanguageHaveWorkedWith`, `TechList`, `PlatformHaveWorkedWith`) stored multi-label data as semi-colon separated strings, leading to thousands of misleading unique entries.\n",
    "\n",
    "- **Solution:**  \n",
    "  Used a systematic loop to apply `str.get_dummies(sep=';')` to each multi-label column, expanding them into individual binary features.\n",
    "\n",
    "- **Result:**  \n",
    "  Discovered actual unique counts such as:\n",
    "  - Languages: ~43\n",
    "  - TechList items: ~58\n",
    "  - Platforms: ~9\n",
    "  - Web frameworks: ~7\n",
    "  - Tools/Tech: ~16\n",
    "  - ProfessionalTech: ~5\n",
    "\n",
    "- **Why this matters:**  \n",
    "  This preserves meaningful multi-label signals (e.g. working with both Python and SQL), while preventing explosion of spurious categories due to string combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca1ca7",
   "metadata": {},
   "source": [
    "### ___Train-Test Split vs Transformation___\n",
    "\n",
    "- **Key Principle:**  \n",
    "  Always split into train & test sets **before fitting transformers**, to avoid data leakage.  \n",
    "  This ensures scalers, encoders, or any learned parameters only learn from the training data.\n",
    "\n",
    "- **EDA Exception:**  \n",
    "  It's acceptable to explore full data correlations & distributions before splitting, since this is purely descriptive and doesn't alter data.\n",
    "\n",
    "- **Plan:**  \n",
    "  1. Finish feature engineering & encoding on entire dataset for understanding.  \n",
    "  2. Once final pipeline is ready, split into train/test.  \n",
    "  3. Fit scalers & encoders on train set, apply them on test set.\n",
    "\n",
    "- **Why this matters:**  \n",
    "  Prevents subtle data leakage that could inflate model performance metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd7369",
   "metadata": {},
   "source": [
    "#### __Analyzing remaining categorical varaibles - Cell 46__\n",
    "\n",
    "- __Decision:__ Variables such as Country and Company size have been analysed to find out how contribute to total salary. For this, a total of 10 most salary-generating countries have been chosen and remaining 171 countries have been dropped to keep the model very specific. A further per-country-salary analysis has been done to find out the relationship between those countries and salaries. \n",
    "- ___Why?___ Total countries were 181, including such big list would end up in fitting the model with too many attributes that do not actally hold much weight in predicting the target.\n",
    "- __Challenges:__ NA\n",
    "\n",
    "To be completed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f911d",
   "metadata": {},
   "source": [
    "#### __YearsCodePro__\n",
    "\n",
    "- __Decision:__ YearsCodePro is of object type, in order to convert it to string, the missing values (almost 20,000+) have to be handled. Scikit learn's SimpleImputer is being used with strategy='mean'. \n",
    "- ___Why?___ It's important to convert YearsCodePro to type integer since it has most numerical values except a few object type ('Less than a year') which are being dropped after analzying their respective impact on the salary. \n",
    "- __Challenges:__ Dropping a label from the column is a bit tricky since the rows are alot and there are 52 unique values. Each low performing value can be dropped to reduce noise in the dataset and for the model. \n",
    "- __Solution:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf601bd",
   "metadata": {},
   "source": [
    "#### __Employment__\n",
    "\n",
    "- __Decision:__ Minimize Employment to certain types to reduce noise and make further analysis possible.  \n",
    "- ___Why?___ There are 106 unique values in the column, including too many ';' separated values which can be easily dropped or bucketed to certain types to reduce the categories and allow for further coorelation analysis with the target. \n",
    "- __Challenges:__ Identifying labels and bucketizing them. \n",
    "- __Solution:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c0c5e",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdd473",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70489d9d",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
