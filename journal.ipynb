{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db55cacd",
   "metadata": {},
   "source": [
    "## ____Stack Overflow Survey ML Project - Learning Journal____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW DATA\n",
    "  # â””â”€â”€> EDA (distributions, correlations, plots, intuition)\n",
    "    #     â””â”€â”€> Preprocessing (handle missing, encode categoricals, bin experience, log salary)\n",
    "      #        â””â”€â”€> Transformers & Pipeline (reusable, scalable, leak-proof)\n",
    "       #             â””â”€â”€> Scaling (StandardScaler)\n",
    "        #                  â””â”€â”€> Modeling (linear regression on log(salary))\n",
    "         #                       â””â”€â”€> Evaluation & Interpretation\n",
    "          #                            â””â”€â”€> Journal documentation & insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b977e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW DATA\n",
    "  # â””â”€â”€> train_test_split\n",
    "    #     â”œâ”€â”€> fit transformers only on train\n",
    "      #   â””â”€â”€> apply transformations on train & test separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238df457",
   "metadata": {},
   "source": [
    "#### ___Project Overview___\n",
    "- Dataset: Stack Overflow 2023 Survey\n",
    "- Goal: __Predicting Yearly Salary (ConvertedComppYearly)__\n",
    "- Key Learning Objectives: Apply Chapter 2 concepts, feature engineering practice, robust pipeline building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff3543",
   "metadata": {},
   "source": [
    "#### ___Target Selection___\n",
    "- **Decision**: ConvertedCompYearly as regression target\n",
    "- **Why**: \n",
    "  - 48K samples, reasonable distribution\n",
    "  - Median $75K aligns with industry knowledge\n",
    "  - Rich feature set for prediction\n",
    "- **Challenges identified**: \n",
    "  - Extreme outliers need handling\n",
    "  - ~46% missing values\n",
    "  - Need currency/location normalization strategy\n",
    "- **Next**: Explore feature relationships and outlier handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07b6a3",
   "metadata": {},
   "source": [
    "####  ___Documentation___\n",
    "\n",
    "#### __Data Exploration__\n",
    "- **Decision**: Checked the dataset for missing values, saw statistical descriptions of each variable.  \n",
    "- **Why**: To find out a reasonable target variable. \n",
    "- **Alternatives considered**: N/A\n",
    "- **Outcome**: Most of the variables are objects/categorical. The model would require clever data preprocessing to find out only relevant variable for the target. Then onwards clever feature engineering would help to build a strong model.\n",
    "\n",
    "#### __Feature Selection__\n",
    "- **Features**: EdLevel, YearsCode, YearsCodePro, DevType, OrgSize, TechList, LanguageHaveWorkedWith, PlatformHaveWorkedWith, WebframeHaveWorkedWith, ToolsTechHaveWorkedWith, WorkExp, Industry, ProfessionalTech.\n",
    "- **Statistics**: 12 out of 13 features are object/categorical mostly highly cordinal. \n",
    "- **Choice**: Based on the tech domain intuition, these features are most likely to relate the most and can be the drivers of tech salaries. \n",
    "- **Challenges**: None so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481cd84",
   "metadata": {},
   "source": [
    "### **Feature Engineering Strategy**\n",
    "\n",
    "<details>\n",
    "  <summary><strong>ðŸ“‚ Click to expand details</strong></summary>\n",
    "\n",
    "---\n",
    "\n",
    "##  Topic: Advanced Feature Engineering for High-Cardinality Categorical Variables\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem Context**\n",
    "Working with the Stack Overflow survey data:\n",
    "- **13 variables** (12 categorical, 1 numerical)\n",
    "- **~89,000 rows**\n",
    "\n",
    "The main challenge:  \n",
    " High-cardinality categorical variables like `TechList` and `LanguageHaveWorkedWith` could explode into **thousands of sparse features** if naÃ¯vely one-hot encoded.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Key Insights Discovered**\n",
    "\n",
    "**1ï¸ The Sparse Matrix Strategy**\n",
    "- **Problem:**  \n",
    "  One-hot encoding all 12 categoricals creates potentially **10K+ features** with **99%+ zeros**.\n",
    "- **Solution:**  \n",
    "  Use sklearnâ€™s sparse matrix support with `feature_names_out` for interpretability.\n",
    "- **Why it works:**  \n",
    "  Sparse matrices store only non-zero values, reducing memory usage by **90-95%**.\n",
    "\n",
    "---\n",
    "\n",
    "**Hierarchical Feature Engineering Approach**\n",
    "Instead of flat one-hot encoding:\n",
    "-  **Stack-level features:** Group technologies into meaningful categories (frontend, backend, data science).\n",
    "-  **Technology-level features:** Preserve granular signals for high-impact individual technologies.\n",
    "\n",
    "---\n",
    "\n",
    "**Salary-Proportional Weighting Strategy**\n",
    "- **Core Problem:**  \n",
    "  Not all technologies within a stack equally impact salary.\n",
    "- **Solution:**  \n",
    "  Weight features **proportionally to salary impact**:\n",
    "\n",
    "- **Rationale:**  \n",
    "Retains genuine salary signals from both high-paying outliers and common technologies.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Technical Decisions Made**\n",
    "\n",
    "- **Weighting Approach:**  \n",
    "Chose raw salary differences over RBF smoothing for now â€” to maintain interpretability.  \n",
    " Will revisit after performance tests.\n",
    "\n",
    "- **Experimental Plan:**  \n",
    "1. Analyze technology â†’ salary relationships  \n",
    "2. Implement proportional weighting  \n",
    "3. Benchmark against simple stack groups  \n",
    "4. Measure correlation & regression metrics  \n",
    "5. Then build a robust transformer for the production pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Links to Chapter 2 Concepts**\n",
    "\n",
    "-  **Feature Engineering Pipelines:**  \n",
    "Following AurÃ©lienâ€™s Ch.2 by separating **experimental exploration from production steps**.\n",
    "\n",
    "-  **Handling Categorical Variables:**  \n",
    "Moving beyond basic one-hots to **domain-driven encodings** that capture business realities.\n",
    "\n",
    "-  **Taming the Curse of Dimensionality:**  \n",
    "Recognizing how high-cardinality categoricals inflate feature space and strategically compressing it.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Next Steps**\n",
    "1. Implement salary analysis per technology  \n",
    "2. Build proportional weighting system  \n",
    "3. Create experimental features & test correlations  \n",
    "4. Compare against baseline stack grouping  \n",
    "5. Document insights before pipeline production.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Key Takeaway**\n",
    "> The best feature engineering blends **domain expertise** (understanding real tech clusters)  \n",
    "> with **statistical rigor** (weighting by salary impact).  \n",
    "> This is where **human judgment becomes irreplaceable** in an ML pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f5dc0",
   "metadata": {},
   "source": [
    "### ___EDA___\n",
    "\n",
    "#####  **Insight**\n",
    "After exploratory scatter plots of `YearsCode` vs `ConvertedCompYearly`, we observed extreme variance and no clear linear relationship.  \n",
    "This confirmed that raw years of experience does **not translate directly into salary** due to multiple hidden confounders (role, location, negotiation, industry).\n",
    "\n",
    "---\n",
    "\n",
    "#####  **Decision**\n",
    "- **1. Log-transform salary**  \n",
    "  To stabilize variance and interpret coefficients in percentage changes, we applied:\n",
    "  \n",
    "  LogSalary = log1p(ConvertedCompYearly)\n",
    "\n",
    "This reduces the impact of extreme salary outliers and makes relationships more linear.\n",
    "\n",
    "- **2. Bucketize years of experience**  \n",
    "Recognizing the diminishing returns of experience, we plan to group `YearsCode` into meaningful categories (e.g., 0-2 yrs, 3-5 yrs, etc).  \n",
    "This approach captures non-linear experience effects and prevents outlier distortion.\n",
    "\n",
    "---\n",
    "\n",
    "#####  **Next step**\n",
    "- Define logical buckets for `YearsCode` and `YearsCodePro` based on both:\n",
    "- **Domain intuition** (typical junior, mid, senior ranges), and\n",
    "- **Actual data distributions** (observed quantiles).\n",
    "- Explore pivot plots of average `LogSalary` vs experience buckets to confirm expected patterns.\n",
    "- Encode these buckets as categorical features in our modeling pipeline.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55bcc89",
   "metadata": {},
   "source": [
    "### _____Handling Multi-Label Categorical Fields_____\n",
    "\n",
    "- **Problem:**  \n",
    "  Several columns (e.g., `LanguageHaveWorkedWith`, `TechList`, `PlatformHaveWorkedWith`) stored multi-label data as semi-colon separated strings, leading to thousands of misleading unique entries.\n",
    "\n",
    "- **Solution:**  \n",
    "  Used a systematic loop to apply `str.get_dummies(sep=';')` to each multi-label column, expanding them into individual binary features.\n",
    "\n",
    "- **Result:**  \n",
    "  Discovered actual unique counts such as:\n",
    "  - Languages: ~43\n",
    "  - TechList items: ~58\n",
    "  - Platforms: ~9\n",
    "  - Web frameworks: ~7\n",
    "  - Tools/Tech: ~16\n",
    "  - ProfessionalTech: ~5\n",
    "\n",
    "- **Why this matters:**  \n",
    "  This preserves meaningful multi-label signals (e.g. working with both Python and SQL), while preventing explosion of spurious categories due to string combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca1ca7",
   "metadata": {},
   "source": [
    "### ___Train-Test Split vs Transformation___\n",
    "\n",
    "- **Key Principle:**  \n",
    "  Always split into train & test sets **before fitting transformers**, to avoid data leakage.  \n",
    "  This ensures scalers, encoders, or any learned parameters only learn from the training data.\n",
    "\n",
    "- **EDA Exception:**  \n",
    "  It's acceptable to explore full data correlations & distributions before splitting, since this is purely descriptive and doesn't alter data.\n",
    "\n",
    "- **Plan:**  \n",
    "  1. Finish feature engineering & encoding on entire dataset for understanding.  \n",
    "  2. Once final pipeline is ready, split into train/test.  \n",
    "  3. Fit scalers & encoders on train set, apply them on test set.\n",
    "\n",
    "- **Why this matters:**  \n",
    "  Prevents subtle data leakage that could inflate model performance metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0391a",
   "metadata": {},
   "source": [
    "### ___Measuring Categorical-Continuous Association & Feature Selection___\n",
    "\n",
    "<details>\n",
    "  <summary><strong>ðŸ“‚ Click to expand details</strong></summary>\n",
    "\n",
    "### ___Categorical Feature Analysis and Selection___\n",
    "Feature Selection: YearsCode\n",
    "The YearsCode feature has been removed from the feature set. Analysis showed a weak and inconsistent relationship between a developer's years of coding experience and their salary. The high variance in salary across all experience levels indicated that this feature added more noise than predictive signal, aligning with the principle of removing weak predictors.\n",
    "\n",
    "Measuring Categorical-Continuous Association\n",
    "To evaluate the relationship between categorical features (e.g., Country, CompanySize) and the continuous target variable (log_salary), we are using statistical tests instead of standard correlation.\n",
    "\n",
    "Methods Used\n",
    "ANOVA F-statistic: This test determines if there are significant differences in the mean salary across the different groups or levels of a categorical feature. A higher F-statistic suggests a stronger relationship.\n",
    "\n",
    "Eta-squared (Î· \n",
    "2\n",
    " ): This metric quantifies the proportion of variance in the salary that is explained by a categorical feature. It provides a more interpretable measure of effect size, ranging from 0 to 1.\n",
    "\n",
    "Î· \n",
    "2\n",
    "  > 0.14: Large effect\n",
    "\n",
    "Î· \n",
    "2\n",
    "  0.06-0.14: Medium effect\n",
    "\n",
    "Î· \n",
    "2\n",
    "  0.01-0.06: Small effect\n",
    "\n",
    "### ___Implementation___\n",
    "\n",
    "A categoricalAssociationAnalysis() function was developed to compute the F-statistic and eta-squared for each categorical feature. This function handles missing values and provides clear association metrics.\n",
    "\n",
    "### ___Next Steps___\n",
    "Analyze Features: Apply the association analysis to the Country, Employment, CompanySize, and EdLevel features.\n",
    "\n",
    "Rank Features: Rank all categorical features based on their eta-squared values.\n",
    "\n",
    "Design Encoding Strategy: Develop a feature encoding plan where features with higher association strength receive more sophisticated handling.\n",
    "\n",
    "\n",
    "\n",
    "<details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd7369",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f911d",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf601bd",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c0c5e",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdd473",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70489d9d",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
