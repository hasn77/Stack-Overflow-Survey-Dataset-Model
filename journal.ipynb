{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db55cacd",
   "metadata": {},
   "source": [
    "## ____Stack Overflow Survey ML Project - Learning Journal____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW DATA\n",
    "  # â””â”€â”€> EDA (distributions, correlations, plots, intuition) - DONE\n",
    "    #     â””â”€â”€> Preprocessing (handle missing, encode categoricals, bin experience, log salary) - DONE\n",
    "      #        â””â”€â”€> Transformers & Pipeline (reusable, scalable, leak-proof) - IN-PROGRESS\n",
    "       #             â””â”€â”€> Scaling (StandardScaler)\n",
    "        #                  â””â”€â”€> Modeling (linear regression on log(salary))\n",
    "         #                       â””â”€â”€> Evaluation & Interpretation\n",
    "          #                            â””â”€â”€> Journal documentation & insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b977e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW DATA\n",
    "  # â””â”€â”€> train_test_split\n",
    "    #     â”œâ”€â”€> fit transformers only on train\n",
    "      #   â””â”€â”€> apply transformations on train & test separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238df457",
   "metadata": {},
   "source": [
    "#### ___Project Overview___\n",
    "- Dataset: Stack Overflow 2023 Survey\n",
    "- Goal: __Predicting Yearly Salary (ConvertedComppYearly)__\n",
    "- Key Learning Objectives: Apply Chapter 2 concepts, feature engineering practice, robust pipeline building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff3543",
   "metadata": {},
   "source": [
    "#### ___Target Selection___\n",
    "- **Decision**: ConvertedCompYearly as regression target\n",
    "- **Why**: \n",
    "  - 48K samples, reasonable distribution\n",
    "  - Median $75K aligns with industry knowledge\n",
    "  - Rich feature set for prediction\n",
    "- **Challenges identified**: \n",
    "  - Extreme outliers need handling\n",
    "  - ~46% missing values\n",
    "  - Need currency/location normalization strategy\n",
    "- **Next**: Explore feature relationships and outlier handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07b6a3",
   "metadata": {},
   "source": [
    "####  ___Documentation___\n",
    "\n",
    "#### __Data Exploration__\n",
    "- **Decision**: Checked the dataset for missing values, saw statistical descriptions of each variable.  \n",
    "- **Why**: To find out a reasonable target variable. \n",
    "- **Alternatives considered**: N/A\n",
    "- **Outcome**: Most of the variables are objects/categorical. The model would require clever data preprocessing to find out only relevant variable for the target. Then onwards clever feature engineering would help to build a strong model.\n",
    "\n",
    "#### __Feature Selection__\n",
    "- **Features**: EdLevel, YearsCode, YearsCodePro, DevType, OrgSize, TechList, LanguageHaveWorkedWith, PlatformHaveWorkedWith, WebframeHaveWorkedWith, ToolsTechHaveWorkedWith, WorkExp, Industry, ProfessionalTech.\n",
    "- **Statistics**: 12 out of 13 features are object/categorical mostly highly cordinal. \n",
    "- **Choice**: Based on the tech domain intuition, these features are most likely to relate the most and can be the drivers of tech salaries. \n",
    "- **Challenges**: Sensitive bucketization is needed for the categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f5dc0",
   "metadata": {},
   "source": [
    "### ___Exploratory Data Analysis___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55bcc89",
   "metadata": {},
   "source": [
    "#### ____Handling Multi-Label Categorical Fields____\n",
    "\n",
    "- **Problem:**  \n",
    "  Several columns (e.g., `LanguageHaveWorkedWith`, `TechList`, `PlatformHaveWorkedWith`) stored multi-label data as semi-colon separated strings, leading to thousands of misleading unique entries.\n",
    "\n",
    "- **Solution:**  \n",
    "  Used a systematic loop to apply `str.get_dummies(sep=';')` to each multi-label column, expanding them into individual binary features.\n",
    "\n",
    "- **Result:**  \n",
    "  Discovered actual unique counts such as:\n",
    "  - Languages: ~43\n",
    "  - TechList items: ~58\n",
    "  - Platforms: ~9\n",
    "  - Web frameworks: ~7\n",
    "  - Tools/Tech: ~16\n",
    "  - ProfessionalTech: ~5\n",
    "\n",
    "- **Why this matters:**  \n",
    "  This preserves meaningful multi-label signals (e.g. working with both Python and SQL), while preventing explosion of spurious categories due to string combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd7369",
   "metadata": {},
   "source": [
    "#### __Analyzing remaining categorical varaibles - Cell 46__\n",
    "\n",
    "- __Decision:__ Variables such as Country and Company size have been analysed to find out how contribute to total salary. For this, a total of 10 most salary-generating countries have been chosen and remaining 171 countries have been dropped to keep the model very specific. A further per-country-salary analysis has been done to find out the relationship between those countries and salaries. \n",
    "- ___Why?___ Total countries were 181, including such big list would end up in fitting the model with too many attributes that do not actally hold much weight in predicting the target.\n",
    "- __Challenges:__ Described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f911d",
   "metadata": {},
   "source": [
    "#### __YearsCodePro__\n",
    "\n",
    "- __Decision:__ YearsCodePro is of object type, in order to convert it to string, the missing values (almost 20,000+) have to be handled. Scikit learn's SimpleImputer is being used with strategy='mean'. \n",
    "- ___Why?___ It's important to convert YearsCodePro to type integer to be able to impute it for handling missing values (using SimpleImputer) since it has most numerical values except a few object type ('Less than a year') which are being dropped after analzying their respective impact on the salary. \n",
    "- __Challenges:__ Dropping a label from the column is a bit tricky since the rows are alot and there are 52 unique values. Each low performing value can be dropped to reduce noise in the dataset and for the model. \n",
    "- __Solution:__ Converted \"Less than a year\" to 0 and dropped \"More than 50 years\" because of very less correlation with the target. SimpleImpter has been used to convert the NaN vlaues to those of the median since the YearsCodePro column was more skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf601bd",
   "metadata": {},
   "source": [
    "#### __Employment__\n",
    "\n",
    "- __Decision:__ Minimize Employment to certain types to reduce noise and make further analysis possible.  \n",
    "- ___Why?___ There are 106 unique values in the column, including too many ';' separated values which can be easily dropped or bucketed to certain types to reduce the categories and allow for further coorelation analysis with the target. \n",
    "- __Challenges:__ Identifying labels and bucketizing them. \n",
    "- __Solution:__ Seperating ';' values first, then bucketizing them into 5 most relative columns. We prioritize full-time employment over other statuses when someone has multiple employment types (like \"Employed, full-time;Independent contractor\"), since full-time employment is likely their primary income source. \n",
    "\n",
    "__Categories:__\n",
    "\n",
    "- Full-time Employed - Traditional full-time jobs\n",
    "- Student - Full-time or part-time students\n",
    "- Freelancer/Self-employed - Independent workers\n",
    "- Part-time Employed - Part-time traditional jobs\n",
    "- Other - Unemployed, retired, not looking, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c0c5e",
   "metadata": {},
   "source": [
    "#### __Bucketizing PlatformsHaveWorkedWith & WebframeHaveWorkedWith__\n",
    "\n",
    "- __Decision:__ Both the columns have ';' separated values, they have been separated by keeping only the first value of the rows as the main value. Missing values have been imputed bsaed on most_frequent values. \n",
    "- ___Why?___ To reduce unnecessary noise and make analysis easier. The values will be bucketized based on tech stack; front-end, back-end, etc. Can be done relative to domain importance as well.\n",
    "- __Challenges:__ NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdd473",
   "metadata": {},
   "source": [
    "#### __Bucketizing WorkExp, EdLevel, DevType, & ProfessionalTech (High Salary Impact)__\n",
    "\n",
    "- __Decision:__ Bucketizing these columns based on domain intuition.\n",
    "- ___Why?___ To reduce noise and also simplify techstacks into fewer groups with equal weights of group values.\n",
    "- __Challenges:__ Challenge described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70489d9d",
   "metadata": {},
   "source": [
    "#### __ISSUE: ProfessionalTech bucket \"None\" with 52272 values__\n",
    "\n",
    "- __Problem:__ After bucketization, \"ProfessionalTech\" columns has 52272 values in the 'None' bucket. These are not NaN values. \n",
    "- ___Why?___ The bucketizaiton rules have to be revised, it can be throwing legitimate values in the \"None\" bracket.\n",
    "- __Solution:__ ProfessionalTech has been bucketed into 7 different buckets. The original column with ';' separated values is  used to extract valuable skills and place them into bucket. A string-strict function is used to classify the 'None' column and it contains all the null (non-answered) values & 'None of these' answers. Unfortunately this makes up more than 50% of the dataset, but the buckets available should work properly in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a62774",
   "metadata": {},
   "source": [
    "#### __Mini-Pipeline__\n",
    "\n",
    "- __Decision:__ A function has been developed which removes ; seperated values intelligently based on a priority approach. It loops through the whole string and selects keys based on priority from the given bucket list. After bucketization, the column is imputed with most_frequent values to fill missing values and is saved in the features column with ending with _Bucket. \n",
    "- ___Why?___ Survey data has many categorical variables with ; separated values which need to follow the Separation > Bucketize > Impution flow. This function does all that in once by taking the dataset, column name, and bucket list as inputs. \n",
    "- __Challenges:__ Previously the process was tiring, this function autmoates the whole process, subsequenly making EDA faster. LanguagesHaveWorkedWith & ToolsTechHaveWorkedWith have been processed properly with very minimal 'None' & 'Other' values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfc7f5",
   "metadata": {},
   "source": [
    "#### __Age, Country, OrgSize & Industry__\n",
    "\n",
    "- __Decision:__ For country, only the top 10 are being kept (based on unique counts) and the rest will be specified as \"Other\" and the ~2500 NaN values would also be handled similarly. Age is alerady in nice age-brackets, it's a perfect usecase for Ordinal Encoding therefore Age is being ordinal encoded for the model to understand i.e. from '18-24 years told' to '5' - assigning a number to each age group while also making sure the model doesn't take number 7 higher than 6 in some cases where Developers above 50 genuinely do not earn higher than the ones under 4O. \n",
    "\n",
    "- OrgSize has more than __~25000 missing values__, and while the basic intuition is that big organizations tend to pay more, the process of handling missing values with most_frequent one can end up making the model mis-classify genuine cases. After analysis, OrgSize column has mostly 'Not Specified' values followed by '20-99' employees. To keep the effect of OrgSize, the NaN values are being imputed by 'Not Specified' for the model to understand and recognize any patters with the values.\n",
    "\n",
    "- Industry column has been dropped to reduce redunant data in the model and to also eliminate unnecesassary noise. There are already sufficient categorical variables to weigh salaries based on experience and expertise.\n",
    "\n",
    "- ___Why?___ The importance of these metrics cannot be undermind in the tech domain, for most of the domain these are important salary drivers and can really affect an individual developer. Hence why a safer approach has been taken for OrgSize to analyse the values and correlation with annual Salary. \n",
    "\n",
    "- __Challenges:__ N/A\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f90b41",
   "metadata": {},
   "source": [
    "### ___FIT ON TRAIN, TRANSFORM ON TEST!___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010fc8f",
   "metadata": {},
   "source": [
    "#### **Feature Engineering Strategy**\n",
    "\n",
    "##### **Problem & Solution**\n",
    "- **Challenge**: 11 high-cardinality categorical variables could create thousands of sparse features with naive one-hot encoding\n",
    "- **Approach**: Hierarchical bucketization (âœ… completed) + salary-impact weighted encoding (ðŸ”„ current focus)\n",
    "- **Result**: Meaningful tech stack categories with proper salary-proportional weights\n",
    "\n",
    "##### **Salary-Proportional Encoding Strategy**\n",
    "- **Core Principle**: Each category value gets weighted based on its actual salary impact, not equal treatment\n",
    "- **Method**: Calculate mean salary per category â†’ encode with salary-impact weights instead of binary 0/1\n",
    "- **Why Critical**: A \"Machine Learning Engineer\" in DevType_Bucket should have higher weight than \"Student\" - encoding must be salary-sensitive\n",
    "\n",
    "##### **Current Implementation Phase**\n",
    "- **Status**: Bucketization complete, moving to transformers with weighted categorical encoding\n",
    "- **Next**: Build robust encoders that assign proper weights to each category value based on salary analysis\n",
    "- **Future**: RBF kernel smoothing for advanced salary impact curves (after baseline pipeline works)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eef1bd",
   "metadata": {},
   "source": [
    "#### __Encoding Strategy__\n",
    "\n",
    "- __Ordinal Encoding:__ `EdLevel_Bucket, ` \n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c24ad",
   "metadata": {},
   "source": [
    "#### __HEADING_HERE__\n",
    "\n",
    "- __Decision:__\n",
    "- ___Why?___\n",
    "- __Challenges:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6fdcb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
